{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "\n",
    "## ğŸ”§ Setup (required once)\n",
    "\n",
    "| Variable / field | Purpose | How to provide |\n",
    "|------------------|---------|----------------|\n",
    "| **USER_AGENT** | Identifies you in HTTP headers when scraping EDGAR. | Export `USER_AGENT=\"Ada <ada@example.com>\"` before launching Jupyter **or** enter one when the setup cell prompts you. |\n",
    "| **OPENAI_API_KEY** | Needed for any cell that calls OpenAI. | Export `OPENAI_API_KEY=\"sk-...\"` **or** paste it when prompted in the setup cell. |\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘  Setup â€“ supply per-user secrets & identifiers     â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "#\n",
    "# Priority for each variable:\n",
    "#   1. Environment variable (best for CI / .env files)\n",
    "#   2. Interactive prompt (OPENAI key) or fallback literal (USER_AGENT)\n",
    "#   3. Users can always overwrite later in the notebook.\n",
    "#\n",
    "import os\n",
    "import sys\n",
    "from getpass import getpass\n",
    "\n",
    "# ---------- USER_AGENT --------------------------------------------\n",
    "USER_AGENT = os.getenv(\"USER_AGENT\")\n",
    "if not USER_AGENT:                      # fall back to a polite prompt\n",
    "    USER_AGENT = input(\n",
    "        \"ğŸ“  Enter a USER_AGENT for EDGAR \"\n",
    "        '(e.g. \"Ada Lovelace <ada@example.com>\"): '\n",
    "    ).strip()\n",
    "    if not USER_AGENT:\n",
    "        sys.exit(\"âŒ  USER_AGENT is required for EDGAR scraping.\")\n",
    "\n",
    "print(f\"âœ… USER_AGENT set to: {USER_AGENT}\")\n",
    "\n",
    "# ---------- OPENAI_API_KEY ----------------------------------------\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "if not OPENAI_API_KEY:                  # secret â†’ use getpass\n",
    "    try:\n",
    "        OPENAI_API_KEY = getpass(\"ğŸ”‘  Paste your OpenAI API key: \")\n",
    "    except EOFError:\n",
    "        sys.exit(\"âŒ OPENAI_API_KEY is required for OpenAI calls.\")\n",
    "\n",
    "# Instantiate a single OpenAI client for reuse\n",
    "from openai import OpenAI\n",
    "client_openai = OpenAI(api_key=OPENAI_API_KEY)\n",
    "print(\"âœ… OpenAI client initialised\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ 1.  Imports                                        â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "# â€”â€”â€” standard library â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "import argparse\n",
    "import datetime as dt\n",
    "import importlib\n",
    "import inspect\n",
    "import json\n",
    "import os\n",
    "import pathlib\n",
    "from pathlib import Path\n",
    "import random\n",
    "import re\n",
    "import time\n",
    "import uuid\n",
    "\n",
    "# â€”â€”â€” third-party â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”\n",
    "import boto3\n",
    "import numpy as np\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import pandas as pd\n",
    "import pytz\n",
    "import requests\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow local helper modules (optional)\n",
    "sys.path.insert(0, \"/tmp\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ 2.  Global configuration                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "RUN_ID   = uuid.uuid4().hex[:8]\n",
    "\n",
    "# Validator tolerance  (Â±5 %)\n",
    "LOWER_TOL_FACTOR = 0.95\n",
    "UPPER_TOL_FACTOR = 1.05\n",
    "\n",
    "# OpenAI\n",
    "completions_model_choice = \"gpt-4o\"\n",
    "OPENAI_API_KEY           = os.getenv(\"OPENAI_API_KEY\")  # or Secrets Manager helper\n",
    "\n",
    "max_tokens_api = 3000\n",
    "retry_max_api = 50\n",
    "temperature_api = 0\n",
    "n_api = 1\n",
    "sleep_time_lower_bound = 1.00\n",
    "sleep_time_upper_bound = 1.10\n",
    "sleep_time = random.uniform(sleep_time_lower_bound, sleep_time_upper_bound)\n",
    "\n",
    "# Cost model\n",
    "MIN_WAGE   = 0.50                      # $ / reviewer-minute\n",
    "MODEL_COST = {\"accept\": 0.001,\n",
    "              \"reject\": 0.001,\n",
    "              \"defer\" : 0.001,\n",
    "              \"unresolved\": 0.001}\n",
    "\n",
    "# â”€â”€ choose where to drop artefacts â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "OUT_DIR = Path.cwd() / \"outputs\"          # always visible in UI\n",
    "OUT_DIR.mkdir(exist_ok=True)              # parents=True not needed here\n",
    "\n",
    "GT_PATH   = OUT_DIR / \"ground_truth.csv\"\n",
    "PASS1_OUT = OUT_DIR / f\"ledger_pass1_{RUN_ID}.jsonl\"\n",
    "PASS2_OUT = OUT_DIR / f\"ledger_pass2_{RUN_ID}.jsonl\"\n",
    "\n",
    "print(\"Working directory :\", Path.cwd())\n",
    "print(\"Artefacts will go :\", OUT_DIR)\n",
    "\n",
    "# Randomised back-off for OpenAI retries\n",
    "sleep_time = random.uniform(1.00, 1.10)\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ 3.  AWS / OpenAI clients                           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "s3 = boto3.client(\"s3\")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ 4.  Static lookup tables & regex helpers           â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "USER_AGENT = \"Your Name <your email>\"\n",
    "CACHE_DIR  = pathlib.Path(\".sec_cache\"); CACHE_DIR.mkdir(exist_ok=True)\n",
    "INPUT_FILE_GROUND_TRUTH = OUT_DIR / \"benchmark_queries_100_FY2021_FY2023.csv\"\n",
    "OUT_FILE_GROUND_TRUTH = OUT_DIR / \"ground_truth.csv\"\n",
    "\n",
    "GAAP_MAP = {\n",
    "    \"Revenue\": [\"Revenues\",\n",
    "                \"RevenueFromContractWithCustomerExcludingAssessedTax\"],\n",
    "    \"Net Income\": [\"NetIncomeLoss\"],\n",
    "    \"EBITDA\": [\"EarningsBeforeInterestTaxesDepreciationAmortization\"],\n",
    "    \"Operating Income\": [\"OperatingIncomeLoss\"],\n",
    "    \"Total Assets\": [\"Assets\"],\n",
    "    \"EPS (diluted)\": [\"EarningsPerShareDiluted\"],\n",
    "    \"Total Equity\": [\"StockholdersEquity\"],\n",
    "    \"Shares Outstanding (diluted)\": [\"WeightedAverageNumberOfDilutedSharesOutstanding\"],\n",
    "    \"Cash From Operations\": [\"NetCashProvidedByUsedInOperatingActivities\"]\n",
    "}\n",
    "CIK_LOOKUP = {\n",
    "    \"AAPL\": \"0000320193\",\n",
    "    \"MSFT\": \"0000789019\",\n",
    "    \"AMZN\": \"0001018724\",\n",
    "    \"NVDA\": \"0001045810\",\n",
    "    \"JPM\":  \"0000019617\",\n",
    "    \"META\": \"0001326801\",\n",
    "    \"GOOGL\": \"0001652044\",\n",
    "    \"BRK.B\": \"0001067983\",\n",
    "    \"KO\":   \"0000021344\",\n",
    "    \"BAC\":  \"0000070858\",\n",
    "}\n",
    "\n",
    "IDK_PATTERNS = [\n",
    "    r\"\\bdo(?:\\s+not|n't)\\s+have\\s+(?:access|the information)\",\n",
    "    r\"\\bno\\s+.*real[- ]time\\s+data\",\n",
    "    r\"\\bi\\s+don't\\s+have\\s+the\\s+specific\",\n",
    "    r\"\\bnot\\s+available\\b\",\n",
    "]\n",
    "\n",
    "UNIT_MULT = {\n",
    "    \"trillion\": 1e12, \"tn\": 1e12, \"t\": 1e12,\n",
    "    \"billion\": 1e9,   \"bn\": 1e9,  \"b\": 1e9,\n",
    "    \"million\": 1e6,   \"mn\": 1e6,  \"m\": 1e6,\n",
    "    \"thousand\": 1e3,  \"k\": 1e3,\n",
    "    \"\": 1.0,\n",
    "}\n",
    "\n",
    "num_re = re.compile(\n",
    "    r\"\"\"\n",
    "    (?P<sign>[-+]?)\\s*\n",
    "    (?P<int>(?:\\d{1,3}(?:[,\\s]\\d{3})+|\\d+))   # <â€‘â€‘ reâ€‘ordered\n",
    "    (?:\\.(?P<frac>\\d+))?\n",
    "    \\s*\n",
    "    (?P<unit>trillion|bn|billion|mn|million|k|thousand|t|m|b)?\n",
    "    \\b\n",
    "    \"\"\",\n",
    "    re.IGNORECASE | re.VERBOSE,\n",
    ")\n",
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ 5.  Pre-allocate ledgers (empty lists)             â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "ledger_pass1 = []\n",
    "ledger_pass2 = []\n",
    "\n",
    "COLOR_PASS1 = \"tab:blue\"\n",
    "COLOR_PASS2 = \"tab:orange\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ask_openai(messages, max_tokens=max_tokens_api, stop=None, retry_max=retry_max_api, model=completions_model_choice):\n",
    "    \"\"\"\n",
    "    Wrapper around OpenAI Chat Completions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    messages : str | list[dict]\n",
    "        Either a single prompt string or a chat-style list with 'role'/'content'.\n",
    "    max_tokens : int\n",
    "        Hard upper-bound on tokens to sample.\n",
    "    stop : list[str] | None\n",
    "        Forwarded to OpenAI to stop generation on any of these strings.\n",
    "    retry_max : int\n",
    "        Retry count on *any* exception.\n",
    "    model : str\n",
    "        Model name to pass through.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    str\n",
    "        Assistant's raw text reply (stripped of outer quotes).\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    * Converts a bare string into `[{\"role\": \"user\", \"content\": ...}]`.\n",
    "    * Sleeps `sleep_time` seconds between retries (simple linear backoff).\n",
    "    * Raises `Exception` if still failing after `retry_max` attempts.\n",
    "    \"\"\"\n",
    "\n",
    "    if isinstance(messages, str):  # If the input is just a single string, wrap it in the appropriate structure.\n",
    "        messages = [{\"role\": \"user\", \"content\": messages}]\n",
    "\n",
    "    response = None\n",
    "    retries = 0\n",
    "    while retries < retry_max:\n",
    "        try:\n",
    "            response = client_openai.chat.completions.create(\n",
    "                model=model,\n",
    "                messages=messages,\n",
    "                max_tokens=max_tokens,\n",
    "                n=1,\n",
    "                stop=stop,\n",
    "                temperature=0\n",
    "            )\n",
    "            # Assuming the response is a pydantic model, we can access attributes directly\n",
    "            content = response.choices[0].message.content.strip('\"')\n",
    "            return content\n",
    "        except Exception as e:  # Catching any exception\n",
    "            print(f\"Error: {e}\")\n",
    "            retries += 1\n",
    "            if retries >= retry_max:\n",
    "                raise Exception(\"API call failed after maximum retries.\")\n",
    "            time.sleep(sleep_time)  # Wait for random seconds before retrying\n",
    "\n",
    "    raise Exception(\"Failed to receive a valid response from OpenAI API.\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# SEC DOWNLOAD + PARSING HELPERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def sec_companyfacts(cik: str) -> dict:\n",
    "    \"\"\"\n",
    "    Download (or read cached) SEC CompanyFacts JSON for a given CIK.\n",
    "\n",
    "    * Pads CIK to 10 digits.\n",
    "    * Caches raw JSON under .sec_cache/CIK##########.json.\n",
    "    * Sleeps `sleep_time` after a network hit to stay polite.\n",
    "    \"\"\"\n",
    "    cik10 = cik.zfill(10)\n",
    "    cache = CACHE_DIR / f\"{cik10}.json\"\n",
    "    if cache.exists():\n",
    "        return json.loads(cache.read_text())\n",
    "    url = f\"https://data.sec.gov/api/xbrl/companyfacts/CIK{cik10}.json\"\n",
    "    r = requests.get(url, headers={\"User-Agent\": USER_AGENT}, timeout=20)\n",
    "    r.raise_for_status()\n",
    "    cache.write_text(r.text)\n",
    "    time.sleep(sleep_time)\n",
    "    return r.json()\n",
    "\n",
    "def best_10k_item(items: list[dict], fy: int):\n",
    "    \"\"\"\n",
    "    Pick the most authoritative FY record from a list of XBRL facts.\n",
    "\n",
    "    Priority order\n",
    "    --------------\n",
    "    1. Exact FY + form == '10-K'\n",
    "    2. Exact FY + form == '10-K/A'\n",
    "    3. If multiple matches, choose the one with the latest 'end' date.\n",
    "    \"\"\"\n",
    "    same_fy = [it for it in items if it.get(\"fy\") == fy]\n",
    "    # split plain vs amended\n",
    "    plain   = [it for it in same_fy if it.get(\"form\") == \"10-K\"]\n",
    "    amended = [it for it in same_fy if it.get(\"form\") == \"10-K/A\"]\n",
    "\n",
    "    def latest(lst):                 # helper returns latest by `end`\n",
    "        return max(lst, key=lambda d: d.get(\"end\", \"0000-00-00\"))\n",
    "\n",
    "    if plain:\n",
    "        return latest(plain)\n",
    "    if amended:\n",
    "        return latest(amended)\n",
    "    return None\n",
    "\n",
    "\n",
    "def fy_value(facts: dict, tags: list[str], fy: int):\n",
    "    \"\"\"\n",
    "    Return (value, unit) for a metric in a specific fiscal year.\n",
    "\n",
    "    Strategy\n",
    "    --------\n",
    "    * Strict FY match using `best_10k_item`.\n",
    "    * If nothing found, fall back to fp=='FY' that ends within calendar year.\n",
    "    * Prefers '10-K' over '10-K/A'; picks latest end-date among duplicates.\n",
    "    \"\"\"\n",
    "    for tag in tags:\n",
    "        unit_dict = facts.get(\"facts\", {}).get(\"us-gaap\", {}).get(tag, {}).get(\"units\", {})\n",
    "        for unit, items in unit_dict.items():\n",
    "            best = best_10k_item(items, fy)\n",
    "            if best:\n",
    "                return best[\"val\"], unit\n",
    "\n",
    "        # ---------- fallback: match by date span -----------------\n",
    "        yr_start, yr_end = dt.date(fy, 1, 1), dt.date(fy, 12, 31)\n",
    "        for unit, items in unit_dict.items():\n",
    "            fy_items = [\n",
    "                it for it in items\n",
    "                if it.get(\"fp\") == \"FY\"\n",
    "                and \"end\" in it\n",
    "                and yr_start <= dt.date.fromisoformat(it[\"end\"]) <= yr_end\n",
    "            ]\n",
    "            if fy_items:\n",
    "                best = max(fy_items, key=lambda d: d[\"end\"])\n",
    "                return best[\"val\"], unit\n",
    "    return None\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# CSV ENRICHMENT\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def enrich(csv_path: Path = INPUT_FILE_GROUND_TRUTH) -> None:\n",
    "    \"\"\"\n",
    "    Populate 'answer' and 'unit' columns in the benchmark CSV, in-place.\n",
    "\n",
    "    * Ensures columns have correct dtypes (float64, string).\n",
    "    * Pulls CompanyFacts for each unique ticker (cached).\n",
    "    * Writes the updated CSV back to `csv_path`.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    # ensure dtype compatibility before we start writing floats / strings\n",
    "    df[\"answer\"] = df.get(\"answer\", pd.Series(index=df.index, dtype=\"float64\")).astype(\"float64\")\n",
    "    df[\"unit\"]   = df.get(\"unit\",   pd.Series(index=df.index, dtype=\"string\")).astype(\"string\")\n",
    "\n",
    "    # cache SEC JSON once per ticker\n",
    "    facts_cache = {tkr: sec_companyfacts(CIK_LOOKUP[tkr])\n",
    "                   for tkr in df[\"ticker\"].unique()}\n",
    "\n",
    "    for idx, row in df.iterrows():\n",
    "        tags = GAAP_MAP.get(row[\"metric\"])\n",
    "        if not tags:\n",
    "            continue\n",
    "        res = fy_value(facts_cache[row[\"ticker\"]], tags, int(row[\"period\"]))\n",
    "        if res:\n",
    "            val, unit = res\n",
    "            df.at[idx, \"answer\"] = val\n",
    "            df.at[idx, \"unit\"]   = unit\n",
    "\n",
    "    df.to_csv(csv_path, index=False)\n",
    "    print(f\"âœ… wrote {csv_path.resolve()}\")\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# NUMERIC EXTRACTION + LLM HELPERS\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def looks_like_idk(text: str) -> bool:\n",
    "    \"\"\"Heuristic: does the draft answer contain an 'I don't know' disclaimer?\"\"\"\n",
    "    return any(re.search(pat, text, re.I) for pat in IDK_PATTERNS)\n",
    "\n",
    "def parse_numeric(text: str) -> float | None:\n",
    "    \"\"\"\n",
    "    Extract first numeric token + magnitude unit, return value in raw USD.\n",
    "\n",
    "    * Recognises trillion / billion / million / thousand suffixes.\n",
    "    * Returns None if pattern or unit missing.\n",
    "    \"\"\"\n",
    "    match = num_re.search(text.replace(\"$\", \"\"))\n",
    "    if not match:\n",
    "        return None\n",
    "\n",
    "    whole = match.group(\"int\").replace(\",\", \"\").replace(\" \", \"\")\n",
    "    frac  = match.group(\"frac\") or \"\"\n",
    "    sign  = -1 if match.group(\"sign\") == \"-\" else 1\n",
    "    val   = float(f\"{whole}.{frac}\") if frac else float(whole)\n",
    "    unit  = (match.group(\"unit\") or \"\").lower()\n",
    "\n",
    "    mult  = UNIT_MULT.get(unit, None)\n",
    "    if mult is None:\n",
    "        return None                      # unknown unit string\n",
    "    return sign * val * mult\n",
    "\n",
    "# ---------- helpers -----------------------------------\n",
    "def generator_llm(q: str) -> str:\n",
    "    \"\"\"Call `ask_openai` with zero-temperature to produce a draft answer.\"\"\"\n",
    "    return ask_openai(\n",
    "        messages=[{\"role\": \"user\", \"content\": q}]\n",
    "    )\n",
    "\n",
    "def referee_llm_det(q: str, draft: str, lo: float, hi: float) -> bool:\n",
    "    \"\"\"\n",
    "    Deterministic validator for numeric queries.\n",
    "\n",
    "    Workflow\n",
    "    --------\n",
    "    1. `looks_like_idk` â†’ 'defer'\n",
    "    2. `parse_numeric` â†’ 'defer' on failure\n",
    "    3. Accept if `lo <= value <= hi`, else 'reject'\n",
    "    \"\"\"\n",
    "    if looks_like_idk(draft):\n",
    "        return \"defer\"             # new state\n",
    "    val = parse_numeric(draft)\n",
    "    #print(f\"[REFEREE] Q='{q[:60]}â€¦'  draft='{draft[:60]}â€¦'  parsed_val={val}\")\n",
    "    if val is None:\n",
    "        return \"defer\"             # couldnâ€™t extract number\n",
    "    return \"accept\" if lo <= val <= hi else \"reject\"\n",
    "\n",
    "def judge_external(row, reason):\n",
    "    \"\"\"\n",
    "    Simulated human fallback (External Judge).\n",
    "\n",
    "    * Always returns authoritative `row.answer`.\n",
    "    * Adds 60 s latency & 1.0 human_min.\n",
    "    * `reason` informs provenance tag.\n",
    "    \"\"\"\n",
    "    answer  = row.answer      # authoritative value\n",
    "    latency = 60              # seconds of expert lookup (simulated)\n",
    "    provenance = f\"fallback:{reason}\"\n",
    "    return answer, 1.0, provenance     # value, human_min, tag\n",
    "\n",
    "def write_memory(query_hash, answer):\n",
    "    \"\"\"\n",
    "    Persist an accepted answer to precedent store (stub in toy demo).\n",
    "    \"\"\"\n",
    "    # stub for future precedent store\n",
    "    pass\n",
    "\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "# KPI + PLOTTING\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def compute_kpi(ledger_path: str, storage_opts=None) -> dict:\n",
    "    \"\"\"\n",
    "    Load a JSONL ledger and compute acceptance / cost metrics.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dict  keys include:\n",
    "        accept_rate, reject_rate, defer_rate, coverage_rate,\n",
    "        avg_latency_s, total_human_min, total_cost_$\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_json(ledger_path, lines=True, storage_options=storage_opts or {})\n",
    "\n",
    "    # pick the latest state column: state, state_p2, state_p3 ...\n",
    "    state_cols = sorted([c for c in df.columns if re.match(r\"state(_p\\d+)?$\", c)])\n",
    "    state_col  = state_cols[-1]\n",
    "\n",
    "    N  = len(df)\n",
    "    counts = df[state_col].value_counts()\n",
    "    accept = (df[state_col] == \"accept\").sum()\n",
    "    reject = (df[state_col] == \"reject\").sum()\n",
    "    defer  = (df[state_col] == \"defer\").sum()\n",
    "    unres  = (df[state_col] == \"unresolved\").sum() if \"unresolved\" in df[state_col].values else 0\n",
    "\n",
    "    # monetise\n",
    "    df[\"dollars\"] = (\n",
    "          df[state_col].map(MODEL_COST).fillna(0)\n",
    "        + df[\"human_min\"] * MIN_WAGE\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"ledger\"          : ledger_path,\n",
    "        \"state_column\"    : state_col,\n",
    "        \"total_queries\"   : N,\n",
    "        \"accept_rate\"     : accept / N,\n",
    "        \"reject_rate\"     : reject / N,\n",
    "        \"defer_rate\"      : defer  / N,\n",
    "        \"unresolved_rate\" : unres  / N,\n",
    "        \"coverage_rate\"   : (accept + reject) / N,\n",
    "        \"avg_latency_s\"   : df.latency_s.mean(),\n",
    "        \"total_human_min\" : df.human_min.sum(),\n",
    "        \"total_cost_$\"    : df.dollars.sum(),\n",
    "    }\n",
    "\n",
    "def plot_comparison(kpi1: dict, kpi2: dict, out_dir: Path | None = None):\n",
    "    \"\"\"\n",
    "    Side-by-side bar charts:\n",
    "\n",
    "    * Chart 1 â€” accept / reject / defer share, coloured consistently.\n",
    "    * Chart 2 â€” total \\$ cost per pass.\n",
    "\n",
    "    Saves PNGs to `out_dir` when provided (README-ready).\n",
    "    \"\"\"\n",
    "    label1, label2 = \"Pass 1\", \"Pass 2\"      # or kpi1[\"ledger\"].stem, â€¦\n",
    "\n",
    "    # â”€â”€ chart 1  (classification mix) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    cats  = [\"accept_rate\", \"reject_rate\", \"defer_rate\"]\n",
    "    y1    = [kpi1[c] for c in cats]\n",
    "    y2    = [kpi2[c] for c in cats]\n",
    "    x     = range(len(cats))\n",
    "    width = 0.35\n",
    "\n",
    "    fig, ax = plt.subplots(figsize=(6, 3), constrained_layout=True)\n",
    "    ax.bar([xi - width/2 for xi in x], y1, width,\n",
    "           label=label1, color=COLOR_PASS1)\n",
    "    ax.bar([xi + width/2 for xi in x], y2, width,\n",
    "           label=label2, color=COLOR_PASS2)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([c.replace(\"_rate\", \"\") for c in cats])\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.set_ylabel(\"fraction\")\n",
    "    ax.set_title(\"Classifications per pass\")\n",
    "    ax.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.18),\n",
    "              ncol=2, frameon=False)\n",
    "\n",
    "    if out_dir:\n",
    "        fig.savefig(out_dir / \"kpi_bar.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "    # â”€â”€ chart 2  (total $ cost) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    fig2, ax2 = plt.subplots(figsize=(4, 3), constrained_layout=True)\n",
    "    ax2.bar(0, kpi1[\"total_cost_$\"], color=COLOR_PASS1, label=label1)\n",
    "    ax2.bar(1, kpi2[\"total_cost_$\"], color=COLOR_PASS2, label=label2)\n",
    "\n",
    "    ax2.set_xticks([0, 1])\n",
    "    ax2.set_xticklabels([label1, label2])\n",
    "    ax2.set_ylabel(\"$ total cost\")\n",
    "    ax2.set_title(\"End-to-end cost\")\n",
    "    ax2.legend(loc=\"upper center\", bbox_to_anchor=(0.5, -0.15),\n",
    "               ncol=2, frameon=False)\n",
    "\n",
    "    if out_dir:\n",
    "        fig2.savefig(out_dir / \"cost_bar.png\", dpi=120, bbox_inches=\"tight\")\n",
    "    plt.show()\n",
    "\n",
    "# â”€â”€â”€ Change in cost / accepted query â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "def cost_per_accept(kpi: dict) -> float | None:\n",
    "    \"\"\"Return $/accept or None if there were zero accepts.\"\"\"\n",
    "    n_acc = int(round(kpi[\"accept_rate\"] * kpi[\"total_queries\"]))\n",
    "    return kpi[\"total_cost_$\"] / n_acc if n_acc else None\n",
    "\n",
    "def pretty_print_kpi(k: dict) -> None:\n",
    "    \"\"\"Readable console dump of key KPI fields + monetised cost.\"\"\"\n",
    "    print(\"\\n== KPI Summary ==\")\n",
    "    for key in [\n",
    "        \"total_queries\", \"accept_rate\", \"reject_rate\", \"defer_rate\",\n",
    "        \"unresolved_rate\", \"coverage_rate\", \"avg_latency_s\",\n",
    "        \"total_human_min\",\n",
    "    ]:\n",
    "        val = k[key]\n",
    "        print(f\"{key:20}: {val:.3f}\" if isinstance(val, float) else\n",
    "              f\"{key:20}: {val}\")\n",
    "    print(f\"\\nEstimated total $ cost: ${k['total_cost_$']:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\"\"\"\n",
    "fill_answers_from_sec.py\n",
    "Populate the 'answer' column of the 100â€‘query benchmark CSV\n",
    "using SEC Companyâ€¯Facts data (10â€‘K).  Tolerates Jupyter's -f flag.\n",
    "\"\"\"\n",
    "\n",
    "ap = argparse.ArgumentParser()\n",
    "ap.add_argument(\"--csv\", default=str(OUT_FILE_GROUND_TRUTH),\n",
    "                help=\"Input benchmark CSV (local path).\")\n",
    "args, _rest = ap.parse_known_args()\n",
    "enrich(Path(args.csv))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ PASS-1  Â·  Deterministic â€œvalidatorâ€ loop                            â•‘\n",
    "# â• â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â•£\n",
    "# â•‘ â€¢ For each benchmark query we:                                      â•‘\n",
    "# â•‘     1. Ask the generator LLM for a draft numeric answer.            â•‘\n",
    "# â•‘     2. Parse that draft deterministically (`referee_llm_det`).      â•‘\n",
    "# â•‘     3. Accept if it falls within Â±5 % of the ground-truth value     â•‘\n",
    "# â•‘        (bounds = answerÂ·LOWER_TOL_FACTOR .. answerÂ·UPPER_TOL_FACTOR)â•‘\n",
    "# â•‘        â€” the validator never sees the exact ground-truth.           â•‘\n",
    "# â•‘     4. Reject âœ call simulated External Judge (adds human cost).    â•‘\n",
    "# â•‘     5. Defer âœ leave unanswered this pass (no judge, no memory).    â•‘\n",
    "# â•‘ â€¢ Each iteration is timed and a JSON-serialisable record is pushed  â•‘\n",
    "# â•‘   to `ledger_pass1`.                                                â•‘\n",
    "# â•‘ â€¢ At the end we write the ledger to  ./outputs/ledger_pass1_<id>.   â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "# ---------- load ground truth -------------------------\n",
    "df_gt = pd.read_csv(GT_PATH)\n",
    "\n",
    "for _, row in df_gt.iterrows():\n",
    "    q, lo, hi = row.query, row.answer * LOWER_TOL_FACTOR, row.answer * UPPER_TOL_FACTOR\n",
    "    t0 = time.time()\n",
    "    draft = generator_llm(q)\n",
    "    state = referee_llm_det(q, draft, lo, hi)\n",
    "\n",
    "    if state == \"accept\":\n",
    "        answer, human_min, prov = draft, 0.0, \"generator\"\n",
    "\n",
    "    elif state == \"reject\":\n",
    "        answer, human_min, prov = judge_external(row, \"reject\")\n",
    "\n",
    "    else:  # state == \"defer\"\n",
    "        answer, human_min, prov = None, 0.0, \"deferred\"\n",
    "        # no judge call, no memory write\n",
    "\n",
    "    # only write memory when we actually have an answer\n",
    "    if answer is not None:\n",
    "        write_memory(hash(q), answer)\n",
    "\n",
    "    ledger_pass1.append(dict(\n",
    "        query_id=int(row.id),\n",
    "        draft=draft,\n",
    "        state=state,\n",
    "        answer=answer,\n",
    "        human_min=human_min,\n",
    "        latency_s=round(time.time()-t0,3),\n",
    "    ))\n",
    "\n",
    "dest_path = OUT_DIR / f\"ledger_pass1_{RUN_ID}.jsonl\"\n",
    "\n",
    "with dest_path.open(\"w\") as f:\n",
    "    for rec in ledger_pass1:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(\"PASS-1 complete; ledger saved to:\", dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ KPI REPORT Â· Pass 1                                              â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘ â€¢ `compute_kpi()` loads the JSONL ledger we just wrote and       â•‘\n",
    "# â•‘   returns a dict of headline metrics (accept/reject/defer rates, â•‘\n",
    "# â•‘   average latency, human-minutes, monetised cost, â€¦).            â•‘\n",
    "# â•‘ â€¢ `pretty_print_kpi()` renders that dict as a human-friendly      â•‘\n",
    "# â•‘   summary block in the notebook output.                          â•‘\n",
    "# â•‘                                                                  â•‘\n",
    "# â•‘   Input  : PASS1_OUT   â†’ ./outputs/ledger_pass1_<RUN_ID>.jsonl   â•‘\n",
    "# â•‘   Output : nicely-formatted KPI table in the cellâ€™s stdout.      â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "kpi1 = compute_kpi(PASS1_OUT)\n",
    "pretty_print_kpi(kpi1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ PASS-2 Â· â€œDefer-fixâ€ sweep                                         â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘ Goal                                                              â•‘\n",
    "# â•‘ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â•‘\n",
    "# â•‘ â€¢ Re-load the Pass-1 ledger.                                      â•‘\n",
    "# â•‘ â€¢ Any query that was *accepted* or *rejected* in Pass-1 is         â•‘\n",
    "# â•‘   copied through unchanged (state_p2 = original state).            â•‘\n",
    "# â•‘ â€¢ Any query that was *deferred* is now sent to the simulated       â•‘\n",
    "# â•‘   External Judge C:                                                â•‘\n",
    "# â•‘       â€“ pull authoritative value from ground_truth CSV            â•‘\n",
    "# â•‘       â€“ mark the new verdict as â€œacceptâ€ (always in-range)        â•‘\n",
    "# â•‘       â€“ add 1.0 human_min   (full expert minute)                  â•‘\n",
    "# â•‘       â€“ add 0.25 s latency  (lookup delay)                        â•‘\n",
    "# â•‘       â€“ provenance tag  =  'fallback:defer_fix'                   â•‘\n",
    "# â•‘       â€“ write answer to precedent store via `write_memory()`      â•‘\n",
    "# â•‘ â€¢ Serialise the resulting list to  ./outputs/ledger_pass2_<id>.   â•‘\n",
    "# â•‘ â€¢ Pass-2 therefore eliminates all defers and yields 100 %         â•‘\n",
    "# â•‘   coverage before we compute KPI_2 / plots.                       â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "ledger_pass1 = pd.read_json(PASS1_OUT, lines=True)\n",
    "gt      = pd.read_csv(GT_PATH).set_index(\"id\")\n",
    "\n",
    "for rec in ledger_pass1.to_dict(\"records\"):\n",
    "    if rec[\"state\"] != \"defer\":\n",
    "        rec[\"state_p2\"] = rec[\"state\"]     # keep old verdict as the pass-2 verdict\n",
    "        ledger_pass2.append(rec)                 # accept / reject unchanged\n",
    "        continue\n",
    "\n",
    "    # --- Judge C supplies authoritative value ---\n",
    "    val = gt.loc[rec[\"query_id\"], \"answer\"]\n",
    "\n",
    "    lo, hi = val*LOWER_TOL_FACTOR, val*UPPER_TOL_FACTOR\n",
    "    state2 = \"accept\" if lo <= val <= hi else \"reject\"   # should be accept\n",
    "\n",
    "    rec.update({\n",
    "        \"state_p2\": state2,\n",
    "        \"answer\": val,\n",
    "        \"human_min\": 1.0,                 # simulated deeper lookup\n",
    "        \"provenance\": \"fallback:defer_fix\",\n",
    "        \"extra_latency_s\": 0.25,\n",
    "    })\n",
    "    write_memory(hash(rec[\"draft\"]), val)\n",
    "    ledger_pass2.append(rec)\n",
    "\n",
    "# dump Pass 2 ledger\n",
    "dest_path = OUT_DIR / f\"ledger_pass2_{RUN_ID}.jsonl\"\n",
    "\n",
    "with dest_path.open(\"w\") as f:\n",
    "    for rec in ledger_pass2:\n",
    "        f.write(json.dumps(rec) + \"\\n\")\n",
    "\n",
    "print(\"PASS-2 complete; ledger saved to:\", dest_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ KPI REPORT Â· Pass 2 (after defer-fix)                            â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘ â€¢ `compute_kpi()` loads the Pass-2 ledger we just wrote and       â•‘\n",
    "# â•‘   recalculates all headline metrics.                             â•‘\n",
    "# â•‘ â€¢ `pretty_print_kpi()` prints the same nicely-formatted summary   â•‘\n",
    "# â•‘   so we can compare directly with the Pass-1 table above.        â•‘\n",
    "# â•‘                                                                  â•‘\n",
    "# â•‘   Input  : PASS2_OUT â†’ ./outputs/ledger_pass2_<RUN_ID>.jsonl     â•‘\n",
    "# â•‘   Output : KPI table for Pass 2 in cell stdout.                  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "kpi2 = compute_kpi(PASS2_OUT)\n",
    "pretty_print_kpi(kpi2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—\n",
    "# â•‘ VISUAL SUMMARY Â· Pass-1  vs  Pass-2                            â•‘\n",
    "# â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•£\n",
    "# â•‘ â€¢ `plot_comparison()` draws two side-by-side figures:           â•‘\n",
    "# â•‘       1.  Bar chart of accept / reject / defer share            â•‘\n",
    "# â•‘           (colours stay consistent across passes).              â•‘\n",
    "# â•‘       2.  Bar chart of total \\$ cost for each pass.             â•‘\n",
    "# â•‘ â€¢ PNGs are saved to  OUT_DIR / {kpi_bar.png, cost_bar.png}      â•‘\n",
    "# â•‘   so they can be embedded in the README, plus rendered inline.  â•‘\n",
    "# â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n",
    "\n",
    "plot_comparison(kpi1, kpi2, out_dir=OUT_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "myenv1",
   "language": "python",
   "name": "myenv1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
